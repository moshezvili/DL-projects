{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# üéØ Home Assignment: AI Agent for Breast Cancer Support\n",
        "\n",
        "## Goal\n",
        "\n",
        "Build an AI agent that interacts with a woman recently diagnosed with breast cancer, asking **personalized and relevant questions** based on her profile. This simulates the kind of **human-centered, empathetic AI assistant**\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Part 1: Build an Empathetic AI Agent\n",
        "\n",
        "Your agent should:\n",
        "\n",
        "1. Ask the user for their **first name**\n",
        "2. Ask for their **age**\n",
        "3. Ask about the **type of breast cancer diagnosis**\n",
        "4. Ask for the **treatment they are currently receiving** (if any)\n",
        "5. Ask if they have a **support system** (e.g., family, friends, community)\n",
        "\n",
        "### üí° Important Nuance\n",
        "\n",
        "The user may not answer on the first try. Your agent must demonstrate:\n",
        "\n",
        "* **Polite persistence**\n",
        "* **Empathy and emotional sensitivity**\n",
        "* The ability to handle **evasive, partial, or delayed responses**\n",
        "\n",
        "---\n",
        "\n",
        "## ‚≠êÔ∏è Part 2: Bonus Challenge ‚Äì Dynamic Dialogue Generation\n",
        "\n",
        "Use a free-form condition summary provided by the user to generate relevant, emotionally sensitive questions.\n",
        "\n",
        "### Sample Input\n",
        "\n",
        "```python\n",
        "condition_profile = {\n",
        "    \"age\": 45,\n",
        "    \"diagnosis\": \"Triple-negative breast cancer\",\n",
        "    \"stage\": \"Stage IIb\",\n",
        "    \"treatment\": \"AC-T chemotherapy just started\",\n",
        "    \"next_steps\": \"Genetic testing and breast MRI\",\n",
        "}\n",
        "```\n",
        "\n",
        "### Your agent should:\n",
        "\n",
        "* Parse this profile intelligently\n",
        "* Generate **5 dynamic, emotionally sensitive questions**\n",
        "* Run the **Q\\&A dialogue** with the user\n",
        "* Account for **hesitancy, deflection, or partial answers**\n",
        "\n",
        "#### ‚ú® Example Questions:\n",
        "\n",
        "* ‚ÄúWould you like to share how you‚Äôre feeling about starting chemotherapy?‚Äù\n",
        "* ‚ÄúHave you been offered genetic counseling yet, or is that still ahead?‚Äù\n",
        "* ‚ÄúDo you have access to information about AC-T and how to manage side effects?‚Äù\n",
        "* ‚ÄúAre you feeling supported emotionally right now?‚Äù\n",
        "* ‚ÄúIs there anything you‚Äôre anxious about in the next few weeks?‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Tools & Libraries\n",
        "\n",
        "You may use any of the following (or others):\n",
        "\n",
        "* OpenAI / Anthropic APIs\n",
        "* LangChain / Haystack\n",
        "* spaCy / NLTK / Hugging Face Transformers\n",
        "* Your own dialogue logic or frameworks\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Submission Guidelines\n",
        "\n",
        "Submit your solution via:\n",
        "\n",
        "* ‚úÖ A Google Colab notebook, **or**\n",
        "* ‚úÖ A GitHub repo, **or**\n",
        "* ‚úÖ A short explainer video + code (if you'd like to demo interactivity)\n",
        "\n",
        "Your code should:\n",
        "\n",
        "* Be **readable and well-organized**\n",
        "* Handle **edge cases** (e.g., missing input, unclear answers)\n",
        "* Demonstrate **empathy** in both wording and flow\n",
        "* Include **brief comments** explaining your design choices\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "RvqFXH4psf1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu6ADbn47cjK",
        "outputId": "662ad019-63ba-4231-eb6d-82c64609fbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.29.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio)\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.29.1-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.1 gradio-client-1.10.1 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.10 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "van2MneSsbJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the notebook use openapi key with name \"open_ai\"\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('open_ai')"
      ],
      "metadata": {
        "id": "ZJAIYu5Q7c6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Solution :\n",
        "prompt + json without structured output"
      ],
      "metadata": {
        "id": "U7aDrGT9nhGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import gradio as gr\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from typing import Optional\n",
        "import os\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Define user profile schema\n",
        "class UserProfile(BaseModel):\n",
        "    name: Optional[str] = Field(None, description=\"First name of the user\")\n",
        "    age: Optional[int] = Field(None, ge=18, le=120, description=\"Age of the user\")\n",
        "    diagnosis: Optional[str] = Field(None, description=\"Type of breast cancer\")\n",
        "    treatment: Optional[str] = Field(None, description=\"Current treatment\")\n",
        "    support: Optional[str] = Field(None, description=\"Support system\")\n",
        "\n",
        "# Initialize state\n",
        "conversation = []\n",
        "user_profile = UserProfile()\n",
        "\n",
        "# System prompt that guides GPT behavior\n",
        "SYSTEM_PROMPT = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": (\n",
        "        \"You are an AI support companion for women recently diagnosed with breast cancer. \"\n",
        "        \"Your primary role is to offer empathetic listening, emotional validation, and compassionate support. \"\n",
        "        \"Woven into your support, you are also tasked with gently gathering specific information needed to potentially tailor future support and resources.\\n\\n\"\n",
        "        \"The key information you need to gather, one piece at a time, includes:\\n\"\n",
        "        \"- **First name**\\n\"\n",
        "        \"- **Age** (Ensure it's within a reasonable adult range)\\n\"\n",
        "        \"- **Type of breast cancer diagnosis**\\n\"\n",
        "        \"- **Current treatment plan** (if any)\\n\"\n",
        "        \"- **Support system** (e.g., family, friends, community, support groups)\\n\\n\"\n",
        "        \"**Interaction Guidelines:**\\n\"\n",
        "        \"1.  **Prioritize Empathy and Support:** Always respond with compassion. Use reflective listening ('It sounds like you're feeling...') and validate their emotions before asking a question or steering the conversation.\\n\"\n",
        "        \"2.  **Ask ONE Question at a Time:** Introduce profile questions naturally and ask for only one piece of information per turn. Do not ask multiple questions in a single response.\\n\"\n",
        "        \"3.  **Gentle Persistence:** If the user doesn't provide a piece of information or changes the subject, find a natural, empathetic way to gently guide the conversation back to the remaining questions *when appropriate* and after addressing their immediate needs.\\n\"\n",
        "        \"4.  **Handle Digressions:** If the user wants to talk about something else or expresses strong feelings, allow them to do so. Listen actively and provide support. Only attempt to ask a profile question or steer back *after* you have validated their feelings and addressed their immediate emotional needs.\\n\"\n",
        "        \"5.  **Clarification:** If an answer is ambiguous or potentially unclear (e.g., 'I'm having chemo' - you might need to gently ask about the *type* if relevant to the fields, or confirm the age if it seems inconsistent), ask a clarifying question before updating the JSON.\\n\"\n",
        "        \"6.  **Never Guess or Fabricate:** Only include information in the JSON that the user has explicitly and clearly provided. If information is missing, leave the corresponding field as `null`.\\n\"\n",
        "        \"7.  **Goal: Complete Profile:** Continue the conversation and gentle inquiry until you have successfully gathered all the required information for the profile fields or the user indicates they cannot or will not provide a specific piece of information.\\n\\n\"\n",
        "        \"**Output Format:**\\n\"\n",
        "        \"After each response to the user, you *must* include the complete current state of the gathered profile information in a JSON block formatted exactly like this:\\n\"\n",
        "        \"```json\\n{\\\"name\\\": null, \\\"age\\\": null, \\\"diagnosis\\\": null, \\\"treatment\\\": null, \\\"support\\\": null}\\n```\\n\"\n",
        "        \"Update the JSON only with information the user explicitly provides and you confirm. Fields for which you have no confirmed information must remain `null`.\\n\\n\"\n",
        "        \"**Initial Message:** Start the conversation with a warm, welcoming, and explanatory message outlining your purpose in a supportive way (e.g., 'Hello, I'm here to listen and support you. To help me understand how I can best assist you, I might gently ask a few questions about your journey...').\"\n",
        "    )\n",
        "}\n",
        "\n",
        "conversation.append(SYSTEM_PROMPT)\n",
        "\n",
        "# Function to extract JSON block from GPT output\n",
        "def extract_json(text):\n",
        "    import json, re\n",
        "    match = re.search(r\"```json\\n(.*?)\\n```\", text, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except json.JSONDecodeError:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "# Function to handle chatbot interaction\n",
        "def chat(user_input):\n",
        "    global user_profile, conversation\n",
        "\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=conversation,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    assistant_message = response.choices[0].message.content\n",
        "    conversation.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "\n",
        "    # Try to extract and validate JSON\n",
        "    extracted = extract_json(assistant_message)\n",
        "    if extracted:\n",
        "        try:\n",
        "            user_profile = UserProfile(**{**user_profile.model_dump(), **extracted})\n",
        "        except ValidationError as e:\n",
        "            print(\"Validation error:\", e)\n",
        "\n",
        "    return assistant_message\n",
        "\n",
        "# Gradio interface\n",
        "def respond(message, history):\n",
        "    reply = chat(message)\n",
        "    return reply\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"\"\"# ü§ñ Empathetic Breast Cancer Support Bot\n",
        "Please talk to the bot. It's here to gently gather a few details and support you emotionally.\n",
        "\"\"\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(placeholder=\"Type here and press Enter...\", label=\"Your Message\")\n",
        "\n",
        "    def handle_input(message, history):\n",
        "        reply = respond(message, history)\n",
        "        history.append((message, reply))\n",
        "        return \"\", history\n",
        "\n",
        "    msg.submit(handle_input, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "bSTPNt1PXLyt",
        "outputId": "95ff02ea-96c6-498d-f9b7-4f2cf4f37153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b5d5bf30d5c8>:97: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://22636b3883f43c4103.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://22636b3883f43c4103.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://22636b3883f43c4103.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second solution:\n",
        "similar approch just with structured output,\n",
        "forcing the llm to return strict json"
      ],
      "metadata": {
        "id": "LtNohestoJ-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata  # or os.environ for local\n",
        "\n",
        "# Init OpenAI\n",
        "client = OpenAI(api_key=userdata.get(\"open_ai\"))\n",
        "\n",
        "# Schema with required chat response\n",
        "class UserProfile(BaseModel):\n",
        "    name: Optional[str] = None\n",
        "    age: Optional[int] = None\n",
        "    diagnosis: Optional[str] = None\n",
        "    treatment: Optional[str] = None\n",
        "    support: Optional[str] = None\n",
        "    chat_response: str  # required for natural flow\n",
        "\n",
        "# Initialize state\n",
        "conversation = []\n",
        "user_profile = UserProfile(chat_response=\"\")\n",
        "\n",
        "# System prompt with behavioral instructions\n",
        "SYSTEM_PROMPT = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": (\n",
        "        \"You are an empathetic AI support agent. Your **primary goal** is to collect the following five fields from the user:\\n\"\n",
        "        \"- name (first name only)\\n\"\n",
        "        \"- age (must be a number)\\n\"\n",
        "        \"- diagnosis (type of breast cancer)\\n\"\n",
        "        \"- treatment (if any)\\n\"\n",
        "        \"- support system (family, friends, community)\\n\\n\"\n",
        "        \"You must gather this information **step by step**.\\n\"\n",
        "        \"Start by asking for their name. Then move to age. Then diagnosis. Then treatment. Then support system.\\n\"\n",
        "        \"At each step:\\n\"\n",
        "        \"- Be warm and emotionally sensitive.\\n\"\n",
        "        \"- If the user avoids the question, gently and politely ask again using different words.\\n\"\n",
        "        \"- Acknowledge feelings, but always try to gather the missing field.\\n\\n\"\n",
        "        \"Once a field is collected, do not ask it again.\\n\"\n",
        "        \"ALWAYS respond in this JSON format:\\n\"\n",
        "        \"{\\n\"\n",
        "        \"  \\\"name\\\": null or string,\\n\"\n",
        "        \"  \\\"age\\\": null or number,\\n\"\n",
        "        \"  \\\"diagnosis\\\": null or string,\\n\"\n",
        "        \"  \\\"treatment\\\": null or string,\\n\"\n",
        "        \"  \\\"support\\\": null or string,\\n\"\n",
        "        \"  \\\"chat_response\\\": \\\"string with your human message\\\"\\n\"\n",
        "        \"}\\n\\n\"\n",
        "        \"Example:\\n\"\n",
        "        \"{\\n\"\n",
        "        \"  \\\"name\\\": \\\"Sarah\\\",\\n\"\n",
        "        \"  \\\"age\\\": null,\\n\"\n",
        "        \"  \\\"diagnosis\\\": null,\\n\"\n",
        "        \"  \\\"treatment\\\": null,\\n\"\n",
        "        \"  \\\"support\\\": null,\\n\"\n",
        "        \"  \\\"chat_response\\\": \\\"Thank you, Sarah. May I ask how old you are?\\\"\\n\"\n",
        "        \"}\"\n",
        "    )\n",
        "}\n",
        "\n",
        "\n",
        "conversation.append(SYSTEM_PROMPT)\n",
        "\n",
        "# Chat logic\n",
        "def chat(user_input):\n",
        "    global user_profile, conversation\n",
        "\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    response = client.beta.chat.completions.parse(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=conversation,\n",
        "        response_format=UserProfile,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    parsed = response.choices[0].message.parsed\n",
        "    merged = {\n",
        "        **user_profile.model_dump(),\n",
        "        **{k: v for k, v in parsed.model_dump().items() if v is not None}\n",
        "    }\n",
        "    user_profile = UserProfile(**merged)\n",
        "\n",
        "    assistant_message = user_profile.chat_response\n",
        "    conversation.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "\n",
        "    # When all fields are complete (excluding chat_response), end flow\n",
        "    if all(getattr(user_profile, f) is not None for f in UserProfile.model_fields if f != \"chat_response\"):\n",
        "        final = (\n",
        "            f\"Thank you so much, {user_profile.name}. You've been very brave sharing all this.\\n\"\n",
        "            \"If there's anything else you‚Äôd like to talk about, I‚Äôm always here for you. üíñ\"\n",
        "        )\n",
        "        conversation.append({\"role\": \"assistant\", \"content\": final})\n",
        "        return final\n",
        "\n",
        "    return assistant_message\n",
        "\n",
        "# Gradio UI\n",
        "def respond(msg, history):\n",
        "    reply = chat(msg)\n",
        "    history.append((msg, reply))\n",
        "    return \"\", history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ü§ñ Empathetic Breast Cancer Support Agent\\n_Gently gathering your information with care._\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(placeholder=\"Share what you're comfortable with...\", label=\"You\")\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIvjiNEZop4X",
        "outputId": "304cfb46-26fb-4b6f-b399-44854bcbfac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-40-e1f90229cdfb>:106: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4974a7c0a7369e2a91.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://4974a7c0a7369e2a91.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bonus Challenge (Dynamic Dialogue Generation)"
      ],
      "metadata": {
        "id": "_wlQvqRmmXNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import openai\n",
        "import gradio as gr\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Any, Optional\n",
        "from pydantic import BaseModel\n",
        "from enum import Enum\n",
        "\n",
        "from google.colab import userdata\n",
        "openai.api_key = userdata.get(\"open_ai\")\n",
        "\n",
        "\n",
        "# 1. Generate personalized, empathetic questions from a patient profile using LLM\n",
        "def generate_questions_from_profile(profile_dict: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generates a list of 5 empathetic questions tailored to a user profile using LLM.\n",
        "    Returns a list of question strings or a fallback list if generation fails.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant specializing in generating empathetic and relevant questions for a support agent interacting with a breast cancer patient.\n",
        "Your task is to create 5 DISTINCT, emotionally sensitive questions based on the provided patient profile. These questions will be used sequentially by the support agent.\n",
        "\n",
        "Each question must:\n",
        "- Be warm, human, and respectful.\n",
        "- Be specifically tailored to the details in THIS user's profile (diagnosis, treatment, next steps, age, etc.).\n",
        "- Focus on feelings, experiences, concerns, or practical support needs, NOT just factual medical details.\n",
        "- Be phrased gently and be open-ended to encourage sharing.\n",
        "- Be suitable to be asked one at a time in a supportive conversation.\n",
        "\n",
        "Output ONLY a JSON list of exactly 5 strings (the questions). Do not include any other text before or after the JSON.\n",
        "\n",
        "Patient Profile:\n",
        "{json.dumps(profile_dict, indent=2)}\n",
        "\"\"\"\n",
        "\n",
        "    fallback_questions = [\n",
        "        \"How are you feeling today?\",\n",
        "        \"Do you have support around you?\",\n",
        "        \"What's been on your mind the most?\",\n",
        "        \"Is there anything specific you're finding challenging?\",\n",
        "        \"What's one small thing that brought you comfort recently?\"\n",
        "    ]\n",
        "\n",
        "    # Schema with required chat response list of 5 strings\n",
        "    class UserQuestions(BaseModel):\n",
        "        questions: List[str]\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Use the standard create method, asking for JSON output\n",
        "        response = openai.beta.chat.completions.parse(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"system\", \"content\": prompt}],\n",
        "            response_format=UserQuestions,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "\n",
        "        parsed = response.choices[0].message.parsed\n",
        "        questions = parsed.questions\n",
        "\n",
        "\n",
        "        if isinstance(questions, list) and len(questions) == 5 and all(isinstance(q, str) for q in questions):\n",
        "            print(\"--- Generated Questions ---\")\n",
        "            for i, q in enumerate(questions):\n",
        "                print(f\"{i+1}. {q}\")\n",
        "            print(\"-------------------------\")\n",
        "            return questions\n",
        "        else:\n",
        "            print(f\"Error: LLM returned incorrect format for questions. Expected list of 5 strings, got: {text}\")\n",
        "            return fallback_questions\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from LLM response (questions): {e}\\nResponse text: {text}\")\n",
        "        return fallback_questions\n",
        "    except Exception as e:\n",
        "        print(f\"An API error occurred during question generation: {e}\")\n",
        "        return fallback_questions\n",
        "\n",
        "# --- Structured Output Definition for Dialogue Turn ---\n",
        "\n",
        "class DialogueAction(str, Enum):\n",
        "    \"\"\"Enum to signal the next state action in the dialogue.\"\"\"\n",
        "    MOVE_NEXT = \"move_next\"         # User addressed current question, move to next\n",
        "    STAY_CURRENT = \"stay_current\"   # User did not address current question, stay on it (rephrase/wait)\n",
        "    CONCLUDE = \"conclude\"           # Dialogue finished (all questions done or conversation ending)\n",
        "\n",
        "class DialogueResponse(BaseModel):\n",
        "    \"\"\"Pydantic model for the structured dialogue turn output.\"\"\"\n",
        "    ai_message: str                 # The natural language message for the user\n",
        "    current_question_addressed: bool # True if the user's last message sufficiently addressed the current question being focused on\n",
        "    next_action: DialogueAction     # The recommended next action for the dialogue state\n",
        "    model_config = {\n",
        "        \"json_schema_extra\": {\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "# --- State Management for the Dialogue ---\n",
        "class ChatState:\n",
        "    \"\"\"Manages the state of the Q&A dialogue.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.questions: List[str] = [] # The list of generated questions\n",
        "        self.current_q_index: int = 0 # Index of the question currently being focused on\n",
        "        self.dialogue_history: List[Dict[str, str]] = [] # Stores the full conversation history for the dialogue LLM\n",
        "\n",
        "# Initialize state globally (common pattern for Gradio state)\n",
        "state = ChatState()\n",
        "\n",
        "# 2. Main Dialogue Handler (using client.beta.chat.completions.parse with Pydantic)\n",
        "def main_dialogue_turn_structured(user_input: str) -> DialogueResponse:\n",
        "    \"\"\"\n",
        "    Processes user input, updates dialogue history, and generates the AI's next response\n",
        "    and state update based on the predefined questions and empathetic persona,\n",
        "    using structured output with Pydantic parse method.\n",
        "\n",
        "    Args:\n",
        "        user_input: The user's message for the current turn.\n",
        "\n",
        "    Returns:\n",
        "        A DialogueResponse Pydantic object containing the AI's message and state signals,\n",
        "        or a fallback DialogueResponse in case of API/parsing errors.\n",
        "    \"\"\"\n",
        "    # Add user message to internal dialogue history\n",
        "    state.dialogue_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Get context for the prompt\n",
        "    current_question = state.questions[state.current_q_index] if state.current_q_index < len(state.questions) else None\n",
        "    next_question_index = state.current_q_index + 1\n",
        "\n",
        "    # --- Prepare the prompt for the dialogue LLM using structured output instructions ---\n",
        "    dialogue_prompt_content = f\"\"\"\n",
        "You are an empathetic support agent assisting a woman recently diagnosed with breast cancer.\n",
        "Your primary role is to provide genuine emotional support, listen actively, and validate feelings.\n",
        "Your secondary goal is to gently guide the conversation to get responses to a list of specific questions ({len(state.questions)} total) to understand the user better and tailor support.\n",
        "\n",
        "You MUST output your response as a JSON object strictly adhering to the Pydantic schema for `DialogueResponse`. The schema is:\n",
        "{json.dumps(DialogueResponse.model_json_schema(), indent=2)}\n",
        "\n",
        "Based on the conversation history, the user's last message, and your list of questions, populate the JSON fields:\n",
        "- `ai_message`: Your complete, natural language response (string). This should be empathetic validation followed by the next conversational turn.\n",
        "- `current_question_addressed`: Boolean indicating if the user's *last message* gave a sufficient response or acknowledgement related to the question you were focused on (Question #{state.current_q_index + 1}).\n",
        "- `next_action`: String enum (`\"move_next\"`, `\"stay_current\"`, `\"conclude\"`).\n",
        "    - `\"move_next\"`: If `current_question_addressed` is true AND there are more questions left. Your `ai_message` should gently lead to Question #{next_question_index + 1}.\n",
        "    - `\"stay_current\"`: If `current_question_addressed` is false. Your `ai_message` should rephrase or offer support around the current question, not ask the next one.\n",
        "    - `\"conclude\"`: If `current_question_addressed` is true AND this was the last question. Your `ai_message` should be a concluding supportive remark.\n",
        "\n",
        "Here is the list of questions you are guiding the user through:\n",
        "\n",
        "<Questions>\n",
        "{chr(10).join([f\"{i+1}. {q}\" for i, q in enumerate(state.questions)])}\n",
        "</Questions>\n",
        "\n",
        "{f\"Your current focus is on Question #{state.current_q_index + 1}: '{current_question}'\" if current_question else \"All questions have been covered. Provide ongoing support if needed.\"}\n",
        "\n",
        "Conversation History (Most Recent Last):\n",
        "{chr(10).join([f\"{msg['role']}: {msg['content']}\" for msg in state.dialogue_history])}\n",
        "\n",
        "Your output MUST be ONLY the JSON object. Do not include any extra text before or after.\n",
        "\"\"\"\n",
        "    # --- Perform the LLM call using the beta parse method ---\n",
        "    try:\n",
        "        # Using the beta parse method with the Pydantic schema\n",
        "        parsed_completion = openai.beta.chat.completions.parse(\n",
        "            model=\"gpt-4o\", # Must use a model that supports json_schema response_format\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": dialogue_prompt_content}\n",
        "            ],\n",
        "            response_format=DialogueResponse, # Pass the Pydantic model directly\n",
        "            temperature=0.7, # Allows for empathetic phrasing variation in ai_message\n",
        "            max_tokens=800 # Increased max tokens slightly to ensure full JSON and message fit\n",
        "        )\n",
        "\n",
        "        parsed_response = parsed_completion.choices[0].message.parsed\n",
        "\n",
        "        # Add AI message from the parsed response to internal history\n",
        "        state.dialogue_history.append({\"role\": \"assistant\", \"content\": parsed_response.ai_message})\n",
        "\n",
        "        return parsed_response # Return the actual Pydantic model instance\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An API error occurred during dialogue turn or parsing: {e}\")\n",
        "        # --- Fallback structured response in case of error ---\n",
        "        fallback_message = \"I'm sorry, I'm having trouble responding right now due to a technical issue. Please feel free to share whatever is on your mind, or we can try returning to the questions later.\"\n",
        "        # Add the fallback message to history\n",
        "        state.dialogue_history.append({\"role\": \"assistant\", \"content\": fallback_message})\n",
        "        # Return a structured response object indicating failure and staying on current question\n",
        "        return DialogueResponse(\n",
        "            ai_message=fallback_message,\n",
        "            current_question_addressed=False, # Assume current question was not addressed if error\n",
        "            next_action=DialogueAction.STAY_CURRENT # Try to stay on the current question/state\n",
        "        )\n",
        "\n",
        "# 3. Gradio Interface Handler\n",
        "def respond(user_text: str, history: List[List[str]], status_text_value: str):\n",
        "    \"\"\"\n",
        "    Handles user input during the dialogue, calls the structured dialogue handler,\n",
        "    updates state based on structured output, and updates Gradio history and status.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not user_text:\n",
        "        # If empty input, just return current state - history unchanged, input text unchanged, status unchanged, interactive unchanged\n",
        "        # We need to return values for outputs=[user_input, chatbox, status_text]\n",
        "        # The input textbox should remain interactive if we are in the respond function\n",
        "        return \"\", history, status_text_value # Return current values\n",
        "\n",
        "    # --- Call Structured Dialogue Logic ---\n",
        "    # Pass the user input to the structured dialogue handler\n",
        "    parsed_response = main_dialogue_turn_structured(user_text)\n",
        "\n",
        "    # --- Update State Based on Structured Output ---\n",
        "    # Use the 'next_action' signal from the LLM to update the question index\n",
        "    if parsed_response.next_action == DialogueAction.MOVE_NEXT:\n",
        "        state.current_q_index += 1\n",
        "    elif parsed_response.next_action == DialogueAction.CONCLUDE:\n",
        "        state.current_q_index = len(state.questions) # Mark as finished by setting index past the end\n",
        "\n",
        "\n",
        "    # --- Determine New Status Message ---\n",
        "    if state.current_q_index >= len(state.questions):\n",
        "        # All questions covered\n",
        "        new_status_value = \"Structured questions covered. You can continue chatting for general support.\"\n",
        "    else:\n",
        "        # Still questions left\n",
        "        new_status_value = f\"Focusing on question {state.current_q_index + 1}/{len(state.questions)}\"\n",
        "\n",
        "    # --- Update Gradio History ---\n",
        "    # Append user message first, then the AI's response from the structured output\n",
        "    # Note: history comes in as a copy in Gradio 4.0, modify and return it\n",
        "    history.append([user_text, None]) # Append user input with placeholder for AI response\n",
        "    history[-1][1] = parsed_response.ai_message # Update the AI's response in the last history entry\n",
        "\n",
        "\n",
        "    # --- Determine Interactive State ---\n",
        "    # The input should remain interactive unless we specifically want to disable it later.\n",
        "    # For this flow, it stays interactive for potential general chat after questions.\n",
        "    user_input_update = gr.update(interactive=True, value=\"\") # Clear input box value, ensure interactive is True\n",
        "\n",
        "\n",
        "    # --- Return Outputs ---\n",
        "    # Return values matching the outputs list: [user_input, chatbox, status_text]\n",
        "    return user_input_update, history, new_status_value\n",
        "\n",
        "\n",
        "# 4. Gradio App Definition\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ü§ñ Personalized Empathetic Support Agent (Dynamic Q&A with Structured Output)\n",
        "    This demo dynamically generates questions based on a profile and guides you through them empathetically using structured AI responses.\n",
        "    \"\"\")\n",
        "\n",
        "    # Chatbox display\n",
        "    chatbox = gr.Chatbot(label=\"Conversation\")\n",
        "\n",
        "    # User input textbox (initially disabled until questions are generated)\n",
        "    user_input = gr.Textbox(label=\"Your Message\", placeholder=\"Type your message and press Enter\", interactive=False)\n",
        "\n",
        "    # Status indicator (informative text like \"Generating questions...\", \"Focusing on question X/Y\", \"Questions covered\")\n",
        "    status_text = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    # --- Event Handlers ---\n",
        "\n",
        "    def start_chat_process():\n",
        "        \"\"\"\n",
        "        Initializes the chat flow upon demo load.\n",
        "        Generates questions, sets initial state, prepares the first message,\n",
        "        and enables the user input box.\n",
        "        Returns values to update the output components.\n",
        "        \"\"\"\n",
        "        # Define the profile to generate questions from\n",
        "        profile = {\n",
        "            \"age\": 45,\n",
        "            \"diagnosis\": \"Triple-negative breast cancer\",\n",
        "            \"stage\": \"Stage IIb\",\n",
        "            \"treatment\": \"AC-T chemotherapy just started\",\n",
        "            \"next_steps\": \"Genetic testing and breast MRI\",\n",
        "            \"note\": \"Patient is likely feeling overwhelmed and anxious about starting chemo and upcoming tests.\" # Added a note for more empathetic tailoring\n",
        "        }\n",
        "\n",
        "        # --- Initialization ---\n",
        "        # Update status text to indicate processing\n",
        "        status_text_update_value = \"Generating personalized questions...\"\n",
        "        # Reset state for a new session\n",
        "        state.__init__()\n",
        "\n",
        "        # --- Generate Questions ---\n",
        "        generated_questions = generate_questions_from_profile(profile)\n",
        "        state.questions = generated_questions\n",
        "        state.current_q_index = 0 # Start with the first question being index 0 (list is 0-indexed)\n",
        "\n",
        "        # --- Prepare Initial Output ---\n",
        "        if not state.questions or state.questions[0].startswith(\"I'm sorry\"):\n",
        "             # Question generation failed or returned fallback list starting with error message\n",
        "             initial_message_value = state.questions[0] # Use the fallback message itself\n",
        "             # Clean up state if generation failed\n",
        "             state.questions = []\n",
        "             state.current_q_index = 0 # Ensure index is 0 if no questions\n",
        "\n",
        "             final_status_value = \"Failed to generate personalized questions. You can chat for general support, but the structured Q&A flow is not active.\"\n",
        "             # Enable user input so they can chat generally\n",
        "             user_input_update = gr.update(interactive=True, value=\"\")\n",
        "             # Start chat history with the AI's initial (error) message\n",
        "             initial_history_value = [(None, initial_message_value)]\n",
        "             # Add to internal dialogue history\n",
        "             state.dialogue_history.append({\"role\": \"assistant\", \"content\": initial_message_value})\n",
        "\n",
        "        else:\n",
        "             # Questions generated successfully\n",
        "             # Set an initial AI welcome message. The first question will be asked by the AI\n",
        "             # in response to the user's *first* input via main_dialogue_turn_structured.\n",
        "             initial_message_value = \"Hello, I'm here to listen and support you. To help me understand how I can best assist you, I might gently ask a few questions about your journey.\"\n",
        "             # Add to internal dialogue history\n",
        "             state.dialogue_history.append({\"role\": \"assistant\", \"content\": initial_message_value})\n",
        "\n",
        "             final_status_value = f\"Questions generated ({len(state.questions)}). We can now begin. Focusing on question {state.current_q_index + 1}/{len(state.questions)}. Please share how you're feeling or respond to the first question when you're ready.\"\n",
        "             # Enable user input and clear any default value\n",
        "             user_input_update = gr.update(interactive=True, value=\"\")\n",
        "             # Start chat history with the AI's welcome message\n",
        "             initial_history_value = [(None, initial_message_value)]\n",
        "\n",
        "        # --- Return Outputs for demo.load ---\n",
        "        # The order of returns must match the order of components in the 'outputs' list below.\n",
        "        # outputs=[chatbox, user_input, status_text]\n",
        "        return initial_history_value, user_input_update, final_status_value\n",
        "\n",
        "\n",
        "    # --- Gradio Event Binding ---\n",
        "    # Initial load event: Triggers the start_chat_process function\n",
        "    # This function updates the chatbox, enables/clears user_input, and updates status_text.\n",
        "    demo.load(\n",
        "        start_chat_process,\n",
        "        inputs=None, # No inputs needed for initial load\n",
        "        outputs=[chatbox, user_input, status_text] # Components to update\n",
        "    )\n",
        "\n",
        "    # User input submission event: Triggers the respond function\n",
        "    # The respond function updates the chat history, clears user_input, and updates status_text.\n",
        "    user_input.submit(\n",
        "        respond,\n",
        "        inputs=[user_input, chatbox, status_text], # Inputs needed: the text, the current history, the current status value\n",
        "        outputs=[user_input, chatbox, status_text] # Components to update\n",
        "    )\n",
        "\n",
        "\n",
        "# Launch the Gradio demo\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "qQWQC_wBHCTi",
        "outputId": "a40164ff-457f-4de7-c76d-d871d402124a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-91b290e8dffd>:273: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbox = gr.Chatbot(label=\"Conversation\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://53165312e9c951d03c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://53165312e9c951d03c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generated Questions ---\n",
            "1. How are you feeling about starting your chemotherapy treatment, and is there anything specific that's been on your mind?\n",
            "2. What concerns or thoughts do you have about the upcoming genetic testing and MRI, and how can I support you through this process?\n",
            "3. With everything going on, how are you managing to find moments of calm or support in your daily life?\n",
            "4. Are there any particular aspects of your treatment or condition that you find most challenging, and how can we help ease those burdens?\n",
            "5. How do you feel about discussing your diagnosis and treatment with friends or family, and do you have the support you need from them?\n",
            "-------------------------\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://53165312e9c951d03c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}