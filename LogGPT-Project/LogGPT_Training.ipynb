{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LogGpt"
      ],
      "metadata": {
        "id": "QES0hy4xeh3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "# Please ensure that uploaded files are available in the AI Studio folder or change the working folder.\n",
        "%cd \"/content/drive/MyDrive/אונ' תל אביב\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYFvAwz3f03-",
        "outputId": "a30016a6-7160-49be-b0d5-7cf72a121413"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/אונ' תל אביב\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepation"
      ],
      "metadata": {
        "id": "BHxsAg36ekQ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n2hHcmZruU4D"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TinyLlama\n",
        "\n",
        "\n",
        "1.   first attempt - train tiny Llama +   tinyllama-function-call-lora-adapter-250424\n",
        "2.   failed - only output with nulls\n",
        "\n"
      ],
      "metadata": {
        "id": "d5NHkYFnPM13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lm-format-enforcer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eck521ax3c4h",
        "outputId": "359df907-1379-4b06-d472-bc2f262af93a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lm-format-enforcer\n",
            "  Downloading lm_format_enforcer-0.10.12-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer) (25.0)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->lm-format-enforcer) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->lm-format-enforcer) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->lm-format-enforcer) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->lm-format-enforcer) (0.4.1)\n",
            "Downloading lm_format_enforcer-0.10.12-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Installing collected packages: interegular, lm-format-enforcer\n",
            "Successfully installed interegular-0.3.3 lm-format-enforcer-0.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from peft     import PeftModel\n",
        "from lmformatenforcer import JsonSchemaParser\n",
        "from lmformatenforcer.integrations.transformers import \\\n",
        "        build_transformers_prefix_allowed_tokens_fn\n",
        "from pydantic import BaseModel\n",
        "from typing   import List, Dict, Optional\n",
        "import torch, json, textwrap\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 1.  load TinyLlama-Chat + LoRA \"function-call\"\n",
        "# ───────────────────────────────────────────────\n",
        "\n",
        "max_seq_length = 4096 * 2\n",
        "dtype = None\n",
        "\n",
        "model, tok = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unclecode/tinyllama-function-call-lora-adapter-250424\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = False,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "id": "0Pp0h34N6apl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 2.  your JSON schema (Pydantic + LMFE)\n",
        "# ───────────────────────────────────────────────\n",
        "class CodeStat(BaseModel):\n",
        "    code:int; count:int|str; bytes:int|str; description:str\n",
        "class Summary(BaseModel):\n",
        "    time_range: str\n",
        "    status_codes: List[CodeStat]\n",
        "    traffic_sources: Optional[Dict[str, str]] = None\n",
        "    resource_types: Optional[Dict[str, str]] = None\n",
        "    admin_activity: Optional[str] = None\n",
        "    observations: Optional[Dict[str, str]] = None\n",
        "    recommendations: Optional[List[str]] = None\n",
        "class LogSynopsis(BaseModel):\n",
        "    summary:Summary\n",
        "\n",
        "parser       = JsonSchemaParser(LogSynopsis.model_json_schema())\n",
        "inner_prefix = build_transformers_prefix_allowed_tokens_fn(tok, parser)\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 3.  build prompt: function-call + log window\n",
        "# ───────────────────────────────────────────────\n",
        "def make_prompt(log_text:str)->str:\n",
        "\n",
        "    # Best practice: Generate this directly from Pydantic to always stay in sync.\n",
        "    # SCHEMA_JSON = LogSynopsis.model_json_schema()\n",
        "    # For clarity, here is the corrected manual version:\n",
        "    SCHEMA_JSON = {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"summary\": {\n",
        "          \"type\": \"object\",\n",
        "          \"properties\": {\n",
        "            \"time_range\": {\"type\": \"string\", \"minLength\": 5},\n",
        "            \"status_codes\": {\n",
        "              \"type\": \"array\", \"minItems\": 1,\n",
        "              \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                  \"code\": {\"type\": \"integer\"},\n",
        "                  \"count\": {\"type\": \"integer\"},\n",
        "                  \"bytes\": {\"type\": \"integer\"},\n",
        "                  \"description\": {\"type\": \"string\", \"minLength\": 1}\n",
        "                },\n",
        "                \"required\": [\"code\", \"count\", \"bytes\", \"description\"]\n",
        "              }\n",
        "            },\n",
        "            # --- Optional fields below ---\n",
        "            \"traffic_sources\": {\"type\": \"object\"},\n",
        "            \"resource_types\": {\"type\": \"object\"},\n",
        "            \"admin_activity\": {\"type\": \"string\"},\n",
        "            \"observations\": {\"type\": \"object\"},\n",
        "            \"recommendations\": {\n",
        "              \"type\": \"array\",\n",
        "              \"items\": {\"type\": \"string\"}\n",
        "            }\n",
        "          },\n",
        "          # ✅ התיקון: רק השדות שהם באמת חובה לפי Pydantic\n",
        "          \"required\": [\n",
        "            \"time_range\",\n",
        "            \"status_codes\"\n",
        "          ]\n",
        "        }\n",
        "      },\n",
        "      \"required\": [\"summary\"]\n",
        "    }\n",
        "\n",
        "\n",
        "    schema_block = textwrap.dedent(f\"\"\"\\\n",
        "        Call `summarize_log` with arguments matching:\n",
        "        {json.dumps(SCHEMA_JSON, indent=2)}\n",
        "\n",
        "    \"\"\")\n",
        "    return f\"{schema_block}\\nLOG>>>\\n{log_text.strip()}\\n<<<\"\n",
        "# דוגמת לוג (קח מהקובץ שלך או קריאה מ-dataset)\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"json\", data_files=\"log_alpaca3.jsonl\", split=\"train\")\n",
        "log_window = ds[0][\"input\"]\n",
        "out = ds[0][\"output\"]\n",
        "prompt     = make_prompt(log_window)\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 4.  tokenise prompt + prefix-fn להגבלת JSON\n",
        "# ───────────────────────────────────────────────\n",
        "batch       = tok([prompt], return_tensors=\"pt\").to(model.device)\n",
        "json_start  = batch[\"input_ids\"].shape[-1]\n",
        "\n",
        "def prefix_fn(batch_id, input_ids):\n",
        "    if input_ids.shape[-1] < json_start:\n",
        "        return list(range(tok.vocab_size))        # חופשי עד סוף-prompt\n",
        "    return inner_prefix(batch_id, input_ids)      # אחרי-כן – רק מה שמותר בסכמה\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 5.  generate\n",
        "# ───────────────────────────────────────────────\n",
        "with torch.no_grad():\n",
        "    out_ids = model.generate(\n",
        "        **batch,\n",
        "        temperature = 0.2,\n",
        "        max_new_tokens = 512,\n",
        "        prefix_allowed_tokens_fn = prefix_fn,\n",
        "        pad_token_id   = tok.eos_token_id,\n",
        "    )\n",
        "\n",
        "reply = tok.decode(out_ids[0][json_start:], skip_special_tokens=True)\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 6.  validate JSON\n",
        "# ───────────────────────────────────────────────\n",
        "data = json.loads(reply)                     # json.loads יזרוק אם לא תקין\n",
        "LogSynopsis.model_validate(data)             # Pydantic validation\n",
        "print(\"\\n✅ Parsed & validated:\", json.dumps(data, indent=2, ensure_ascii=False))\n",
        "print(\"---\"*100)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8IYl2Xo3iQY",
        "outputId": "99be4055-35bc-42a7-fdd2-471167c707a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Parsed & validated: {\n",
            "  \"summary\": {\n",
            "    \"status_codes\": [],\n",
            "    \"observations\": null,\n",
            "    \"traffic_sources\": null,\n",
            "    \"time_range\": \"\"\n",
            "  }\n",
            "}\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "{\"summary\":{\"time_range\":\"28/Jul/2025\",\"status_codes\":[{\"code\":200,\"count\":19,\"description\":\"Successful requests\"},{\"code\":301,\"count\":7,\"description\":\"Moved Permanently\"},{\"code\":302,\"count\":6,\"description\":\"Found (Temporary Redirect)\"}],\"traffic_sources\":null,\"resource_types\":null,\"admin_activity\":null,\"observations\":null,\"recommendations\":[\"Monitor the frequency of 301 redirects to ensure they are necessary.\",\"Review the user agent strings for accuracy and relevance.\"]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pu8Rz-sc-RkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train TinyLlama from scartch\n"
      ],
      "metadata": {
        "id": "-MpEnrZ0-bwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Optional\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define the maximum sequence length for the model\n",
        "max_seq_length = 8192\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 1. Define the Pydantic Schema and Tool Structure\n",
        "# ───────────────────────────────────────────────\n",
        "class CodeStat(BaseModel):\n",
        "    code:int\n",
        "    count:int\n",
        "    bytes:int\n",
        "    description:str\n",
        "\n",
        "class Summary(BaseModel):\n",
        "    time_range: str\n",
        "    status_codes: List[CodeStat]\n",
        "    traffic_sources: Optional[Dict[str, str]] = None\n",
        "    resource_types: Optional[Dict[str, str]] = None\n",
        "    admin_activity: Optional[str] = None\n",
        "    observations: Optional[Dict[str, str]] = None\n",
        "    recommendations: Optional[List[str]] = None\n",
        "\n",
        "class LogSynopsis(BaseModel):\n",
        "    summary:Summary\n",
        "\n",
        "# Define the full tool structure with a name.\n",
        "LOG_SUMMARY_TOOL = {\n",
        "    \"name\": \"log_summary_tool\",\n",
        "    \"description\": \"Extracts a structured summary from a block of web server log text.\",\n",
        "    \"parameters\": LogSynopsis.model_json_schema()\n",
        "}\n",
        "TOOLS_LIST_STR = json.dumps([LOG_SUMMARY_TOOL], indent=4)\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 2. Load the BASE model and add a NEW adapter\n",
        "# ───────────────────────────────────────────────\n",
        "# We start from the original base chat model for a clean fine-tune.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/tinyllama-bnb-4bit\", # Use the specified 4-bit model\n",
        "    max_seq_length = max_seq_length,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# We add a fresh LoRA adapter for our specific task.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        "    max_seq_length = max_seq_length,\n",
        ")\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 3. Prepare the Dataset with the Correct Prompt Format\n",
        "# ───────────────────────────────────────────────\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "\n",
        "    system_instruction = \"You are a helpful assistant with access to the following functions. Use them if required -\"\n",
        "\n",
        "    for log_text, output_str in zip(inputs, outputs):\n",
        "        try:\n",
        "            parsed_arguments = json.loads(output_str)\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Skipping invalid JSON in output: {output_str}\")\n",
        "            continue\n",
        "\n",
        "        full_tool_call_output = {\n",
        "            \"name\": \"log_summary_tool\",\n",
        "            \"arguments\": parsed_arguments\n",
        "        }\n",
        "        clean_output_json = json.dumps(full_tool_call_output)\n",
        "\n",
        "        # This is the standard chat template for TinyLLaMA\n",
        "        system_prompt = f\"<|system|>\\n{system_instruction}\\n{TOOLS_LIST_STR}</s>\"\n",
        "        user_prompt = f\"<|user|>\\n{log_text}</s>\"\n",
        "        assistant_response = f\"<|assistant|>\\n<functioncall> {clean_output_json}</s>\"\n",
        "\n",
        "        full_text = system_prompt + \"\\n\" + user_prompt + \"\\n\" + assistant_response\n",
        "        texts.append(full_text)\n",
        "\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# ✅ CORRECTED: Load both JSONL files into a single dataset\n",
        "data_files = [\"log_alpaca3.jsonl\", \"log_alpaca.jsonl\"]\n",
        "dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True,)\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 4. Define Training Arguments\n",
        "# ───────────────────────────────────────────────\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 5,       # ✅ Use epochs instead of max_steps for stability\n",
        "        learning_rate = 2e-5,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 5. Start the Fine-Tuning Process\n",
        "# ───────────────────────────────────────────────\n",
        "print(\"Starting fine-tuning from base model...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete!\")\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 6. Save the new adapter\n",
        "# ───────────────────────────────────────────────\n",
        "print(\"Saving the new LoRA adapter to 'my-log-finetune-lora'...\")\n",
        "model.save_pretrained(\"my-log-finetune-lora\")\n",
        "print(\"Adapter saved! You can now use the updated inference script.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3d556aaca9034edbb6bd3a66584d93c4",
            "1a3a997fbf944cd99dfbc4053c765c63",
            "97ab14698ecf4feea0880a4fe055b32b",
            "4834ba92d6b541b6b4a973718bbcbba7",
            "cdd94f07bcfd458092600a02206b11da",
            "c3f0ccf7b62e4de2b9b3230a0edfab07",
            "d794b845de14495e96cf06e27c969415",
            "6107989c5b6945998bf2ec4ffb126ec5",
            "b9864c3fdd594c5fb603ae783d056ded",
            "91cb219f6aba4a809743cd5edc08eb6e",
            "9fdfb7c530fa449c8f2557b359318e1f",
            "3169e02b833d436c862fed06bda4deb1",
            "19b288112c2b467082abebb4173504f5",
            "fb46c453648e4a3496ee5a1493689b62",
            "83b3c03b15df445e897d3fb07727b9a1",
            "a0fb4f961cd547a49fa278be3637a497",
            "6277708742134ff1af80348423e1fddc",
            "dc701ffecbb4419b9830e3e7cfcca6b2",
            "870c79369033440b9195f0c380f8cf56",
            "f71abdfeaaef4e7a9fdba110b4eaa7bc",
            "24092b1fec4e45ce97aecb823c4ec1eb",
            "f38e45b0558d4821a10220ec558dcaa9"
          ]
        },
        "id": "mD8jk1cjDdr8",
        "outputId": "82253dac-ff68-4afe-8813-83c6311abc5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.8.1: Fast Llama patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d556aaca9034edbb6bd3a66584d93c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3169e02b833d436c862fed06bda4deb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning from base model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 600 | Num Epochs = 5 | Total steps = 375\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 12,615,680 of 1,112,664,064 (1.13% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 10:50, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.667900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.667900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.668900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.668200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.667300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.665200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.663100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.659600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.656500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.652700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.648500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.644200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.637700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.632900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.623700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.619500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.611000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.603800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.596100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.589200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.582700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.573300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.565300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.557200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.550100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.541400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.533100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.526500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.517900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.509900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.502100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.494800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.486200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.478300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.469600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.461200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.451500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.445100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.435000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.428600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.419800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.410300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.403300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.394100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.386300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.378200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.368800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.362200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.353100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.344000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.335200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.326400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.317800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.310900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.301400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.292500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.283500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.274500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.266900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.258300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.247600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.239500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.231800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.222700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.215500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.208200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.199200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.192200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.182800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.176300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.168900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.160500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.153300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.147400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.135800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.129100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.122300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.119900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.112700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.107300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.103800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.099400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.095600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.091800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.086000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.082100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.077300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.074900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.071600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.067100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.064000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.060100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.056600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.054000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.051900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.049600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.043400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.044300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.038000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.036500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.035000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.035100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.033900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.032900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.030300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.030100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.030200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.033300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.031200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.031100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.030500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.027300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.028700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.025800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.026300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.025500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.027200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.024600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.025700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.024800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.023700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.024800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.022600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.022300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.024600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.023600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.021900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.023700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.024600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.022600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.024500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.023100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.021200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.021800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.021600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.021300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.023700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.020900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.020900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.021600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.021100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.020100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.019400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.022300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.022100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.021300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.019400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.021800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.019500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.019200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.019300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.021600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.019400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.021000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.019400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.020300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.018100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.018600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.021500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.021100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.021000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.020500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.020100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.017300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.020700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.018500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.018100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.017300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.018500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.018100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.016900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.018000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.016700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.017800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.019000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.020800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.018700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.021000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.018200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.017800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.017500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.020200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.020100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.019500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.016700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.017900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.019100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.017200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.018300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.018200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.017600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.018900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.016300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.019900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.019000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.015100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>0.019100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.017000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>0.018300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.018100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.018100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.015900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.016900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.017900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.017200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>0.018300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.019100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>0.017900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.016100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.016500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.016000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.017400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>0.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.017300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.017900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.015400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>0.017100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>0.019100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.015200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.017900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.018200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>0.017900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.018400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.018300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.017200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.014000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.015800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>0.016600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.015500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.017800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.017200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.016200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.015900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.016200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.016500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>0.016700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.015500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>0.015400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.016800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.016500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.014900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.017000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.015500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.015500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>0.016300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.017500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>0.016800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.018100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.014900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.016900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.016000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.014500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.017100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>0.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.016200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.016000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.016500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.015200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.016000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>0.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.015800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>0.013800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.015800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.014900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.016600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>0.016000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.015000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.012700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.016600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>0.015100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.015900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>0.013500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>0.016100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>0.013800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>0.015200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.014500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.015900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>0.014900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>0.015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.013700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>0.014800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>0.015000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.013300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>0.015100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.014500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.014600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.014300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>0.014000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.013800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>0.014200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.014100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>0.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>0.014500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>0.014000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>0.014100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>0.013600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>0.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>0.015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.012100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>0.014300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.012300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>0.013900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>0.014300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>0.013800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>0.014100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.012600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>0.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>0.014300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>0.014300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>0.013700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>0.013600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>0.014200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>0.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.013200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>0.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>0.013600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>0.013600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>0.013600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.014200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>0.015100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>0.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>0.012200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>0.014900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.013300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.013100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>0.015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>0.014000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>0.014800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>0.014600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.014300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>0.013800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>0.012400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.014200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>0.011600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>0.015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>0.011300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>0.015500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.015900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>0.014500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.014100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>0.013800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>0.014600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>0.014000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>0.016100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>0.014000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>0.014900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.016500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>0.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>0.013300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>0.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.013700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>0.012700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>0.015100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>0.013800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>0.010700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.013800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>0.014200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>0.014300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>0.014300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.013800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.014400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning complete!\n",
            "Saving the new LoRA adapter to 'my-log-finetune-lora'...\n",
            "Adapter saved! You can now use the updated inference script.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import textwrap\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Optional, Any\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from lmformatenforcer import JsonSchemaParser\n",
        "from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 1. Load your FINE-TUNED model\n",
        "# ───────────────────────────────────────────────\n",
        "# This loads the adapter that you just trained and saved.\n",
        "max_seq_length = 8192\n",
        "dtype = None # None for auto-detection\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"my-log-finetune-lora\", # ✅ Load your custom adapter!\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 2. Define Pydantic Schemas & Tool Structure\n",
        "# ───────────────────────────────────────────────\n",
        "# These schemas define the structure of the *arguments* for our tool.\n",
        "class CodeStat(BaseModel):\n",
        "    code:int\n",
        "    count:int\n",
        "    bytes:int\n",
        "    description:str\n",
        "\n",
        "class Summary(BaseModel):\n",
        "    time_range: str\n",
        "    status_codes: List[CodeStat]\n",
        "    traffic_sources: Optional[Dict[str, str]] = None\n",
        "    resource_types: Optional[Dict[str, str]] = None\n",
        "    admin_activity: Optional[str] = None\n",
        "    observations: Optional[Dict[str, str]] = None\n",
        "    recommendations: Optional[List[str]] = None\n",
        "\n",
        "class LogSynopsis(BaseModel):\n",
        "    summary:Summary\n",
        "\n",
        "# Define the full tool structure that the model expects.\n",
        "LOG_SUMMARY_TOOL = {\n",
        "    \"name\": \"log_summary_tool\",\n",
        "    \"description\": \"Extracts a structured summary from a block of web server log text.\",\n",
        "    \"parameters\": LogSynopsis.model_json_schema()\n",
        "}\n",
        "TOOLS_LIST_STR = json.dumps([LOG_SUMMARY_TOOL], indent=4)\n",
        "\n",
        "# Create a Pydantic model for the *entire tool call* output.\n",
        "# This is what the lm-format-enforcer will validate against.\n",
        "class ToolCall(BaseModel):\n",
        "    name: str = Field(..., description=\"The name of the function to call.\")\n",
        "    arguments: LogSynopsis = Field(..., description=\"The arguments to pass to the function.\")\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 3. Setup the Format Enforcer\n",
        "# ───────────────────────────────────────────────\n",
        "# The parser now uses the schema of the full ToolCall.\n",
        "parser = JsonSchemaParser(ToolCall.model_json_schema())\n",
        "prefix_enforcer = build_transformers_prefix_allowed_tokens_fn(tokenizer, parser)\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 4. Build the Inference Prompt\n",
        "# ───────────────────────────────────────────────\n",
        "# This prompt now perfectly matches the standard TinyLLaMA format\n",
        "# used in the corrected training script.\n",
        "def make_inference_prompt(log_text:str)->str:\n",
        "    system_instruction = \"You are a helpful assistant with access to the following functions. Use them if required -\"\n",
        "\n",
        "    system_prompt = f\"<|system|>\\n{system_instruction}\\n{TOOLS_LIST_STR}</s>\"\n",
        "    user_prompt = f\"<|user|>\\n{log_text}</s>\"\n",
        "    assistant_prompt = f\"<|assistant|>\\n<functioncall> \" # We prompt it to start the function call\n",
        "\n",
        "    return system_prompt + \"\\n\" + user_prompt + \"\\n\" + assistant_prompt\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 5. Run Inference\n",
        "# ───────────────────────────────────────────────\n",
        "# Load the same dataset to get an example log to test with\n",
        "ds = load_dataset(\"json\", data_files=\"log_alpaca3.jsonl\", split=\"train\")\n",
        "log_window = ds[0][\"input\"]\n",
        "ground_truth_output = json.loads(ds[0][\"output\"])\n",
        "\n",
        "# Create the prompt and tokenize it\n",
        "prompt = make_inference_prompt(log_window)\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "# Generate the output, using the format enforcer as a safety rail\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens = 1024,\n",
        "        prefix_allowed_tokens_fn = prefix_enforcer,\n",
        "        pad_token_id = tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "# Decode only the newly generated tokens\n",
        "newly_generated_tokens = outputs[0][prompt_len:]\n",
        "json_part = tokenizer.decode(newly_generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 6. Validate and Compare the Output\n",
        "# ───────────────────────────────────────────────\n",
        "print(\"✅ Model Generated JSON:\")\n",
        "try:\n",
        "    json_part = json_part.replace(tokenizer.eos_token, \"\").strip()\n",
        "    # First, validate the entire tool call structure\n",
        "    tool_call_data = ToolCall.model_validate_json(json_part)\n",
        "\n",
        "    # If successful, print the nicely formatted arguments\n",
        "    # ✅ CORRECTED: Switched from deprecated .dict() to .model_dump()\n",
        "    print(json.dumps(tool_call_data.arguments.model_dump(), indent=2, ensure_ascii=False))\n",
        "\n",
        "except (json.JSONDecodeError, ValueError) as e:\n",
        "    print(f\"Error parsing or validating generated JSON: {e}\")\n",
        "    print(f\"Raw model output: {json_part}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"---\"*20 + \"\\n\")\n",
        "\n",
        "print(\"🎯 Ground Truth JSON:\")\n",
        "print(json.dumps(ground_truth_output, indent=2, ensure_ascii=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk4CvDx7Iclx",
        "outputId": "b127ca12-627d-4afd-af3b-80fe12d12397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.8.1: Fast Llama patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "✅ Model Generated JSON:\n",
            "{\n",
            "  \"summary\": {\n",
            "    \"time_range\": \"- \",\n",
            "    \"status_codes\": [],\n",
            "    \"traffic_sources\": {\n",
            "      \"TSTSTSTSTS  Jones - - - - Jones \": \") \"\n",
            "    },\n",
            "    \"resource_types\": {\n",
            "      \" }, \": \" \",\n",
            "      \" \": \", . \"\n",
            "    },\n",
            "    \"admin_activity\": \" \",\n",
            "    \"observations\": {\n",
            "      \" \": \" \"\n",
            "    },\n",
            "    \"recommendations\": [\n",
            "      \" \",\n",
            "      \" \",\n",
            "      \" Jones Jones Jones Jones Jones \",\n",
            "      \" \",\n",
            "      \" \",\n",
            "      \" \",\n",
            "      \" \",\n",
            "      \" \",\n",
            "      \" \",\n",
            "      \" \",\n",
            "      \" \",\n",
            "      \" \",\n",
            "      \" \",\n",
            "      \" Jones \",\n",
            "      \" Jones \",\n",
            "      \" \",\n",
            "      \" \",\n",
            "      \" \",\n",
            "      \" } \",\n",
            "      \" \"\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "🎯 Ground Truth JSON:\n",
            "{\n",
            "  \"summary\": {\n",
            "    \"time_range\": \"28/Jul/2025\",\n",
            "    \"status_codes\": [\n",
            "      {\n",
            "        \"code\": 200,\n",
            "        \"count\": 19,\n",
            "        \"description\": \"Successful requests\"\n",
            "      },\n",
            "      {\n",
            "        \"code\": 301,\n",
            "        \"count\": 7,\n",
            "        \"description\": \"Moved Permanently\"\n",
            "      },\n",
            "      {\n",
            "        \"code\": 302,\n",
            "        \"count\": 6,\n",
            "        \"description\": \"Found (Temporary Redirect)\"\n",
            "      }\n",
            "    ],\n",
            "    \"traffic_sources\": null,\n",
            "    \"resource_types\": null,\n",
            "    \"admin_activity\": null,\n",
            "    \"observations\": null,\n",
            "    \"recommendations\": [\n",
            "      \"Monitor the frequency of 301 redirects to ensure they are necessary.\",\n",
            "      \"Review the user agent strings for accuracy and relevance.\"\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train strognger Model - llama-3-8b-bnb-4bit\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9GJdIZ7Dh1EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Optional\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define the maximum sequence length for the model\n",
        "max_seq_length = 8192\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 1. Define the Pydantic Schema and Tool Structure\n",
        "# ───────────────────────────────────────────────\n",
        "class CodeStat(BaseModel):\n",
        "    code:int\n",
        "    count:int\n",
        "    bytes:int\n",
        "    description:str\n",
        "\n",
        "class Summary(BaseModel):\n",
        "    time_range: str\n",
        "    status_codes: List[CodeStat]\n",
        "    traffic_sources: Optional[Dict[str, str]] = None\n",
        "    resource_types: Optional[Dict[str, str]] = None\n",
        "    admin_activity: Optional[str] = None\n",
        "    observations: Optional[Dict[str, str]] = None\n",
        "    recommendations: Optional[List[str]] = None\n",
        "\n",
        "class LogSynopsis(BaseModel):\n",
        "    summary:Summary\n",
        "\n",
        "# Define the full tool structure with a name.\n",
        "LOG_SUMMARY_TOOL = {\n",
        "    \"name\": \"log_summary_tool\",\n",
        "    \"description\": \"Extracts a structured summary from a block of web server log text.\",\n",
        "    \"parameters\": LogSynopsis.model_json_schema()\n",
        "}\n",
        "TOOLS_LIST_STR = json.dumps([LOG_SUMMARY_TOOL], indent=4)\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 2. Load the Llama 3 8B BASE model and add a NEW adapter\n",
        "# ───────────────────────────────────────────────\n",
        "# ✅ CORRECTED: Switched to a more powerful base model.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        ")\n",
        "\n",
        "# We add a fresh LoRA adapter for our specific task.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        "    max_seq_length = max_seq_length,\n",
        ")\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 3. Prepare the Dataset with the Correct Llama 3 Prompt Format\n",
        "# ───────────────────────────────────────────────\n",
        "# ✅ CORRECTED: Switched back to the Llama 3 chat template.\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "\n",
        "    system_instruction = \"You are a helpful assistant with access to the following functions. Use them if required -\"\n",
        "\n",
        "    for log_text, output_str in zip(inputs, outputs):\n",
        "        try:\n",
        "            parsed_arguments = json.loads(output_str)\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Skipping invalid JSON in output: {output_str}\")\n",
        "            continue\n",
        "\n",
        "        full_tool_call_output = {\n",
        "            \"name\": \"log_summary_tool\",\n",
        "            \"arguments\": parsed_arguments\n",
        "        }\n",
        "        clean_output_json = json.dumps(full_tool_call_output)\n",
        "\n",
        "        # This is the standard chat template for Llama 3\n",
        "        prompt = f\"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
        "        prompt += f\"{system_instruction}\\n{TOOLS_LIST_STR}<|eot_id|>\"\n",
        "        prompt += f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "        prompt += f\"{log_text}<|eot_id|>\"\n",
        "        prompt += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        full_text = f\"{prompt}<functioncall>{clean_output_json}<|eot_id|>\"\n",
        "        texts.append(full_text)\n",
        "\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Load both JSONL files into a single dataset\n",
        "data_files = [\"log_alpaca3.jsonl\", \"log_alpaca.jsonl\"]\n",
        "dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True,)\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 4. Define Training Arguments\n",
        "# ───────────────────────────────────────────────\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 3, # 3 epochs is a good starting point for a larger model\n",
        "        learning_rate = 2e-5,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 5. Start the Fine-Tuning Process\n",
        "# ───────────────────────────────────────────────\n",
        "print(\"Starting fine-tuning from Llama 3 8B base model...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete!\")\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 6. Save the new adapter\n",
        "# ───────────────────────────────────────────────\n",
        "print(\"Saving the new LoRA adapter to 'my-log-finetune-lora'...\")\n",
        "model.save_pretrained(\"my-log-finetune-lora\")\n",
        "print(\"Adapter saved! You can now use the updated inference script.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0c2697e820c34b019c623720a10cbcc0",
            "82528d8dc31443579bdc71eae80040f2",
            "602ea9ec849349e0b0ed38db70cd99dc",
            "9f41e02349164665a1758503318789d1",
            "4b85f5af96e443cf92ea7e3f9bc06478",
            "7488490ae8324f55bc2f89fa1e1a9b84",
            "1fa3b699b67a40349ce8852f1bb805cc",
            "bb21023e9f174c6daaa02c2a548e3e10",
            "c0610d27235c4118a4c172134d8ea6df",
            "5ee6d59fa7e84ecea65664fa8bc36e5c",
            "136e5af781054aa2a744eebdb2a5439f",
            "aaf4443a35f84d46bdcac689123e13eb",
            "b4363317321542bd96355bf20e53b4b8",
            "97d587e70f354e9283279fa21b23eb01",
            "8bb620a0e9c0490b84cd661a83f6f4b3",
            "8dca6878872b4b3caa1278918b177306",
            "a5a5ce91620543dc830c7d663da43d96",
            "37301b37eac44b26b10aea5f2bc3dfcf",
            "6db22d65137c4947a163bb578a9a7703",
            "01d4a0ae40e34411b38098053b56785e",
            "8cf47fbec38a4f54987be6812aa90af9",
            "769c09c3a4564092bb28c15a8f142055"
          ]
        },
        "id": "KRuORvAuf4RP",
        "outputId": "8a2a1ba9-8d66-43a7-f65b-486ca5eba648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.8.1: Fast Llama patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c2697e820c34b019c623720a10cbcc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aaf4443a35f84d46bdcac689123e13eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning from Llama 3 8B base model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 600 | Num Epochs = 3 | Total steps = 225\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [225/225 36:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.197300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.969500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.035200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.909300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.031400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.078500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.008400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.072100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.953100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.985600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.920700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.980800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.987600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.888600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.817000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.825700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.864600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.802000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.800200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.909600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.806400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.810700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.797300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.757200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.826800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.794700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.789500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.637600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.767900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.710100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.608300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.611400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.606700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.587100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.545800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.485200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.606200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.578700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.589700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.502500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.583300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.496600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.433800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.459200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.563400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.469600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.597800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.501600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.382000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.383000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.470200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.370800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.456600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.374200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.462500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.461000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.382000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.343900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.381700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.299900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.368600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.324700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.526900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.423700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.431500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.337600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.440800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.323900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.337300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.405900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.319700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.401900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.312100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.431700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.411700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.301000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.360600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.401600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.270900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.326700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.334400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.502300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.265900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.382300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.315900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.413600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.286600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.372200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.362400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.242000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.397800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.340600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.408300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.388300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.359500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.251700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.404700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.358800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.450900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.385200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.388100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.311200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.488100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.376600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.278600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.417500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.264300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.305800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.321900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.386800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.341600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.289800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.308100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.316600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.339800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.384100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.313800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.346400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.240400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.381100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.356500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.340600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.324800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.401200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.308400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.375200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.305800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.375200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.307300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.299600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.353000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.296400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.349400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.318300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.369900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.343300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.310300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.330700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.371000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.342200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.356900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.348800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.290500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.250100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.384100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.249200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.417500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.304700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.381700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.285100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.285100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.330200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.318400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.337800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.214400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.351200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.305600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.324700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.448000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.357300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.360700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.291000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.334500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.323900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.276600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.352700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.310400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.328200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.382700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.266700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.325300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.325000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.311200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.349100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.411300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.285100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.262800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.301500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.355200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.326900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.326800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.247900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.304500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.355400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.305400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.346800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.269000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.318500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.263400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.307100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.292600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.352700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.285600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.317600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.315100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.333100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.279200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.364300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.273100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.360500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>0.371200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.291100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>0.246000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.256000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.310000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>0.352500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.267000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.325000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.260100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>0.356600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.469700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>0.238900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.349700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.350600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.305900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.388300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>0.344500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.407700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.310900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.295200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>0.326000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>0.237500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.224600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning complete!\n",
            "Saving the new LoRA adapter to 'my-log-finetune-lora'...\n",
            "Adapter saved! You can now use the updated inference script.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import textwrap\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Optional, Any\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from lmformatenforcer import JsonSchemaParser\n",
        "from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 1. Load your FINE-TUNED model\n",
        "# ───────────────────────────────────────────────\n",
        "# This loads the adapter that you just trained and saved.\n",
        "max_seq_length = 8192\n",
        "dtype = None # None for auto-detection\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = \"my-log-finetune-lora\", # ✅ Load your custom Llama 3 adapter!\n",
        "#     max_seq_length = max_seq_length,\n",
        "#     rope_scaling= 4,\n",
        "#     dtype = dtype,\n",
        "#     load_in_4bit = True,\n",
        "# )\n",
        "# FastLanguageModel.for_inference(model)\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 2. Define Pydantic Schemas & Tool Structure\n",
        "# ───────────────────────────────────────────────\n",
        "# These schemas define the structure of the *arguments* for our tool.\n",
        "class CodeStat(BaseModel):\n",
        "    code:int\n",
        "    count:int\n",
        "    bytes:int\n",
        "    description:str\n",
        "\n",
        "class Summary(BaseModel):\n",
        "    time_range: str\n",
        "    status_codes: List[CodeStat]\n",
        "    traffic_sources: Optional[Dict[str, str]] = None\n",
        "    resource_types: Optional[Dict[str, str]] = None\n",
        "    admin_activity: Optional[str] = None\n",
        "    observations: Optional[Dict[str, str]] = None\n",
        "    recommendations: Optional[List[str]] = None\n",
        "\n",
        "class LogSynopsis(BaseModel):\n",
        "    summary:Summary\n",
        "\n",
        "# Define the full tool structure that the model expects.\n",
        "LOG_SUMMARY_TOOL = {\n",
        "    \"name\": \"log_summary_tool\",\n",
        "    \"description\": \"Extracts a structured summary from a block of web server log text.\",\n",
        "    \"parameters\": LogSynopsis.model_json_schema()\n",
        "}\n",
        "TOOLS_LIST_STR = json.dumps([LOG_SUMMARY_TOOL], indent=4)\n",
        "\n",
        "# Create a Pydantic model for the *entire tool call* output.\n",
        "# This is what the lm-format-enforcer will validate against.\n",
        "class ToolCall(BaseModel):\n",
        "    name: str = Field(..., description=\"The name of the function to call.\")\n",
        "    arguments: LogSynopsis = Field(..., description=\"The arguments to pass to the function.\")\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 3. Setup the Format Enforcer\n",
        "# ───────────────────────────────────────────────\n",
        "# The parser now uses the schema of the full ToolCall.\n",
        "parser = JsonSchemaParser(ToolCall.model_json_schema())\n",
        "prefix_enforcer = build_transformers_prefix_allowed_tokens_fn(tokenizer, parser)\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 4. Build the Inference Prompt\n",
        "# ───────────────────────────────────────────────\n",
        "# This prompt now perfectly matches the Llama 3 format used in training.\n",
        "def make_inference_prompt(log_text:str)->str:\n",
        "    system_instruction = \"You are a helpful assistant with access to the following functions. Use them if required -\"\n",
        "\n",
        "    prompt = f\"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
        "    prompt += f\"{system_instruction}\\n{TOOLS_LIST_STR}<|eot_id|>\"\n",
        "    prompt += f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "    prompt += f\"{log_text}<|eot_id|>\"\n",
        "    prompt += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    prompt += f\"<functioncall>\" # We prompt it to start the function call\n",
        "    return prompt\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 5. Run Inference\n",
        "# ───────────────────────────────────────────────\n",
        "# Load the same dataset to get an example log to test with\n",
        "ds = load_dataset(\"json\", data_files=\"log_alpaca3.jsonl\", split=\"train\")\n",
        "log_window = ds[2][\"input\"]\n",
        "ground_truth_output = json.loads(ds[0][\"output\"])\n",
        "\n",
        "# Create the prompt and tokenize it\n",
        "prompt = make_inference_prompt(log_window)\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "# Generate the output, using the format enforcer as a safety rail\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        temperature = 0.1,\n",
        "        max_new_tokens = 1024,\n",
        "        prefix_allowed_tokens_fn = prefix_enforcer,\n",
        "        pad_token_id = tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "# Decode only the newly generated tokens\n",
        "newly_generated_tokens = outputs[0][prompt_len:]\n",
        "json_part = tokenizer.decode(newly_generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 6. Validate and Compare the Output\n",
        "# ───────────────────────────────────────────────\n",
        "print(\"✅ Model Generated JSON:\")\n",
        "try:\n",
        "    # The output from the model should be the JSON content of the function call\n",
        "    json_part = json_part.replace(\"<|eot_id|>\", \"\").strip()\n",
        "\n",
        "    # First, validate the entire tool call structure\n",
        "    tool_call_data = ToolCall.model_validate_json(json_part)\n",
        "\n",
        "    # If successful, print the nicely formatted arguments\n",
        "    print(json.dumps(tool_call_data.arguments.model_dump(), indent=2, ensure_ascii=False))\n",
        "\n",
        "except (json.JSONDecodeError, ValueError) as e:\n",
        "    print(f\"Error parsing or validating generated JSON: {e}\")\n",
        "    print(f\"Raw model output: {json_part}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"---\"*20 + \"\\n\")\n",
        "\n",
        "print(\"🎯 Ground Truth JSON:\")\n",
        "print(json.dumps(ground_truth_output, indent=2, ensure_ascii=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3QBDng-hIv8",
        "outputId": "fc76c048-ade7-488f-93e0-bd89a8923a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model Generated JSON:\n",
            "Error parsing or validating generated JSON: 1 validation error for ToolCall\n",
            "  Invalid JSON: control character (\\u0000-\\u001F) found while parsing a string at line 30 column 1193 [type=json_invalid, input_value='{\"arguments\" : {\"summary...DDDDD*DD**, <DDDD: <* >', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "Raw model output: {\"arguments\" : {\"summary\" :   {\"status_codes\"  : \t   \n",
            "\n",
            " \t   [] \t   \t   \t,\"admin_activity\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\t\t\t:\" \" ,\"observations\" \t   \t   \n",
            "\n",
            "\n",
            ": null \t   \n",
            " \t   \n",
            ",\"resource_types\" \n",
            ": null ,\"time_range\"\t\t\t\t\t\t\t\t\n",
            "\n",
            "\n",
            "\n",
            ":\" \"\n",
            "\n",
            "\n",
            "\t\t\t\t\t\t\t\t\t,\"recommendations\"\n",
            "\t\t\t\t\t\t\t\t\t\t\t:\t\t\t    [ ]\t    \t\t\t\t\t\t\t}\t\t\t\t\t\t\t   \t\t}   \t\t\t\t\t\t\t\t\t,\"name\"\n",
            "\n",
            "\n",
            "\n",
            "   \t   \t:\" Jones ( Jones Jones Jones Jones Jones < Jones < < < < < <TS < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <<< < < < < < < < < < <<<< < < < < < < <TP < < < <TPs <TP <TP <TP < <TP <TP * * < * < < < <s < <s <s', <s},},},},},},},}, ', <<<<<<<<<<<<<<<<< < < < < < < < << < <<<< < < <<<<< < < <s <s <s <s <s <s <<<< <<<<<},}, < < < < <<< < < < < << < < < < <<<< < < <<<<<<<<<< • < • <s <s < • <s < • < • < • < • • • • < • < • • •<<<<<<<<< < < < <<<<<<<<<< <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<s <s <s < • <s <s <s <s <s <s <s <s <<s<s <s <<s <s <s <<N<<S<S<S<S<<<<<<S <S<S<S<N<s <s <S<N<O<O<O<O <S<N<O<S<O<O<O<NSSSSSOOOSSSSSSSSSs>sOSOOSOSsOes*s*s>s>s>s*s>s>s<s>s>s>s>s>S* > > > > > > > > > > > • • • • • • •SSS*S‡ • • • •S•S••*•*S•** • <S<S<s>s<<<•••••<••••••<S<S<S<S<<<<<<<<S<<<<<S<<<D<D<S<S<S<S<<<@@@@ <@s@s@s <s < < < <<<s < < < <<<<<<<<<Ds<<* > <@ > <@**@*@@**@M*** > <@* <* <@<S* to to to <D<<<<<<<<<<D<DDDDDDDDDDDDDDDDDDD** <DDDDDDDDDDDDDDDDD<D<DDDD<DDDDD,\tDDDDDD, <,\tD,\tDDDDDD*DD**, <DDDD: <* >\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "🎯 Ground Truth JSON:\n",
            "{\n",
            "  \"summary\": {\n",
            "    \"time_range\": \"28/Jul/2025\",\n",
            "    \"status_codes\": [\n",
            "      {\n",
            "        \"code\": 200,\n",
            "        \"count\": 19,\n",
            "        \"description\": \"Successful requests\"\n",
            "      },\n",
            "      {\n",
            "        \"code\": 301,\n",
            "        \"count\": 7,\n",
            "        \"description\": \"Moved Permanently\"\n",
            "      },\n",
            "      {\n",
            "        \"code\": 302,\n",
            "        \"count\": 6,\n",
            "        \"description\": \"Found (Temporary Redirect)\"\n",
            "      }\n",
            "    ],\n",
            "    \"traffic_sources\": null,\n",
            "    \"resource_types\": null,\n",
            "    \"admin_activity\": null,\n",
            "    \"observations\": null,\n",
            "    \"recommendations\": [\n",
            "      \"Monitor the frequency of 301 redirects to ensure they are necessary.\",\n",
            "      \"Review the user agent strings for accuracy and relevance.\"\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Try - diffrent data sturcture format"
      ],
      "metadata": {
        "id": "tu8caAUmidKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Optional\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define the maximum sequence length for the model\n",
        "max_seq_length = 8192\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 1. Define the Pydantic Schema (No changes needed)\n",
        "# ───────────────────────────────────────────────\n",
        "class CodeStat(BaseModel):\n",
        "    code:int\n",
        "    count:int\n",
        "    bytes:int\n",
        "    description:str\n",
        "\n",
        "class Summary(BaseModel):\n",
        "    time_range: str\n",
        "    status_codes: List[CodeStat]\n",
        "    traffic_sources: Optional[Dict[str, str]] = None\n",
        "    resource_types: Optional[Dict[str, str]] = None\n",
        "    admin_activity: Optional[str] = None\n",
        "    observations: Optional[Dict[str, str]] = None\n",
        "    recommendations: Optional[List[str]] = None\n",
        "\n",
        "class LogSynopsis(BaseModel):\n",
        "    summary:Summary\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 2. Load the Llama 3 8B BASE model and add a NEW adapter\n",
        "# ───────────────────────────────────────────────\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        "    max_seq_length = max_seq_length,\n",
        ")\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 3. Prepare the Dataset with the SIMPLIFIED Alpaca Prompt Format\n",
        "# ───────────────────────────────────────────────\n",
        "# ✅ CORRECTED: Switched to a simple, direct instruction format.\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# We still need the schema to pass into the instruction.\n",
        "LOG_SYNOPSIS_SCHEMA = LogSynopsis.model_json_schema()\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "\n",
        "    instruction = (\n",
        "        \"You are NetLogGPT. Summarize the following log window and respond ONLY with a valid JSON object that conforms to this Pydantic schema:\\n\"\n",
        "        f\"{json.dumps(LOG_SYNOPSIS_SCHEMA, indent=2)}\"\n",
        "    )\n",
        "\n",
        "    for log_text, output_str in zip(inputs, outputs):\n",
        "        try:\n",
        "            # The output from the dataset needs to be a clean JSON string\n",
        "            parsed_output = json.loads(output_str)\n",
        "            clean_output_json = json.dumps(parsed_output)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "        full_text = alpaca_prompt.format(\n",
        "            instruction,\n",
        "            log_text,\n",
        "            clean_output_json + tokenizer.eos_token # Add EOS token to signal end of generation\n",
        "        )\n",
        "        texts.append(full_text)\n",
        "\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Load both JSONL files into a single dataset\n",
        "data_files = [\"log_alpaca3.jsonl\", \"log_alpaca.jsonl\"]\n",
        "dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True,)\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 4. Define Training Arguments\n",
        "# ───────────────────────────────────────────────\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10, # Increased warmup for a larger dataset/model\n",
        "        num_train_epochs = 4,\n",
        "        learning_rate = 1e-5,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "\n",
        "\n",
        "    ),\n",
        ")\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 5. Start the Fine-Tuning Process\n",
        "# ───────────────────────────────────────────────\n",
        "print(\"Starting fine-tuning with simplified Alpaca format...\")\n",
        "trainer.train(resume_from_checkpoint=True)\n",
        "print(\"Fine-tuning complete!\")\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 6. Save the new adapter\n",
        "# ───────────────────────────────────────────────\n",
        "print(\"Saving the new LoRA adapter to 'my-log-finetune-lora'...\")\n",
        "model.save_pretrained(\"my-log-finetune-lora-new\")\n",
        "print(\"Adapter saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f61003362cb148d885c34d117406a849",
            "a882591d45794281b3eac3f2c35a8ad8",
            "f4aea2e24d8d428696c95bac5dfb88bf",
            "882d22e8e371405683c2cc384d3aa0d7",
            "3587e91e2f1e4095a83f49fc4d835043",
            "cabfc6113b804dc3b915c343767f8aff",
            "16693553f2bb4b70b6e66e5bdeea94b5",
            "f012c36b3c9f492189a04ea8ce59652c",
            "8369ffd1e0c54ecbaad83d1b8906ef36",
            "0206706f64cf450f99544c7b98d7fb69",
            "7d6282a78ce94db3968a44478a74f347",
            "94925eaa47454426a3476974ea546ee8",
            "4e94209c90c24b5e92a5c22f36f2f8b9",
            "c8319ca8919448e9a716b1d0fd6be162",
            "d1579f3a145145b1a44ed6b151c0ad2e",
            "d2208d6381374101a5ec3837af6037e2",
            "d4f57eb88e944591895ec91737614f24",
            "86626dd9c5074a5b99f74263e0f3e82a",
            "d4e4edb02a87413ba2092ea2a35c71f9",
            "7f9ab95ae18a41e993e43e77d6fedcdc",
            "4eb0ee830f48415baef625703937998b",
            "0d52f4e223c94567a63bb4b17326ce19"
          ]
        },
        "id": "uQpfZ3tfqkhk",
        "outputId": "45b93946-2456-4264-c754-d3563af4b003"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.8.1: Fast Llama patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f61003362cb148d885c34d117406a849"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94925eaa47454426a3476974ea546ee8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning with simplified Alpaca format...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 600 | Num Epochs = 4 | Total steps = 300\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 12:31, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>0.587400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.638100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.552600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.572300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.585200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.633300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>0.531800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.543800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.510700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.591600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.576600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.540200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.596400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.468900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>0.567500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>0.578300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.528600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.535900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.604700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.554800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.549300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.495200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.542500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>0.465300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.599700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>0.500900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.402000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.482700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.536300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.469000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.541800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.539400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.468500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>0.481500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.446000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.548000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.439000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.465500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.408600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>0.376900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.407100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>0.483500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.501800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.446600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.534600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>0.538600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.477100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.497500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.455800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>0.357600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.479100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>0.497600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.410400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>0.372000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>0.473200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>0.378000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.415500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.450400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>0.416900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>0.369600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.383000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>0.370300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.383100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>0.479200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.490100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>0.345500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.382000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.354800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.406600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>0.435500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.506300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>0.524900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.421700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning complete!\n",
            "Saving the new LoRA adapter to 'my-log-finetune-lora'...\n",
            "Adapter saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Optional\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from lmformatenforcer import JsonSchemaParser\n",
        "from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 1. Load your FINE-TUNED model\n",
        "# ───────────────────────────────────────────────\n",
        "# This loads the adapter that you just trained with the Alpaca format.\n",
        "max_seq_length = 8192\n",
        "dtype = None # None for auto-detection\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"my-log-finetune-lora-new\", # ✅ Load your custom Llama 3 adapter!\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 2. Define Pydantic Schemas\n",
        "# ───────────────────────────────────────────────\n",
        "class CodeStat(BaseModel):\n",
        "    code:int\n",
        "    count:int\n",
        "    bytes:int\n",
        "    description:str\n",
        "\n",
        "class Summary(BaseModel):\n",
        "    time_range: str\n",
        "    status_codes: List[CodeStat]\n",
        "    traffic_sources: Optional[Dict[str, str]] = None\n",
        "    resource_types: Optional[Dict[str, str]] = None\n",
        "    admin_activity: Optional[str] = None\n",
        "    observations: Optional[Dict[str, str]] = None\n",
        "    recommendations: Optional[List[str]] = None\n",
        "\n",
        "class LogSynopsis(BaseModel):\n",
        "    summary:Summary\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 3. Setup the Format Enforcer\n",
        "# ───────────────────────────────────────────────\n",
        "# The parser now directly enforces the LogSynopsis schema, as we are no longer using a tool/function call structure.\n",
        "parser = JsonSchemaParser(LogSynopsis.model_json_schema())\n",
        "prefix_enforcer = build_transformers_prefix_allowed_tokens_fn(tokenizer, parser)\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 4. Build the Inference Prompt\n",
        "# ───────────────────────────────────────────────\n",
        "# This prompt now perfectly matches the Alpaca format used in the final training script.\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "LOG_SYNOPSIS_SCHEMA = LogSynopsis.model_json_schema()\n",
        "\n",
        "def make_inference_prompt(log_text:str)->str:\n",
        "    instruction = (\n",
        "        \"You are NetLogGPT. Summarize the following log window and respond ONLY with a valid JSON object that conforms to this Pydantic schema:\\n\"\n",
        "        f\"{json.dumps(LOG_SYNOPSIS_SCHEMA, indent=2)}\"\n",
        "    )\n",
        "    return alpaca_prompt.format(\n",
        "        instruction,\n",
        "        log_text,\n",
        "        \"\", # The response to be generated\n",
        "    )\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 5. Run Inference\n",
        "# ───────────────────────────────────────────────\n",
        "# Load the same dataset to get an example log to test with\n",
        "ds = load_dataset(\"json\", data_files=\"log_alpaca3.jsonl\", split=\"train\")\n",
        "log_window = ds[2][\"input\"]\n",
        "ground_truth_output = json.loads(ds[2][\"output\"])\n",
        "\n",
        "# Create the prompt and tokenize it\n",
        "prompt = make_inference_prompt(log_window)\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "# Generate the output, using the format enforcer as a safety rail\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens = 512,\n",
        "        prefix_allowed_tokens_fn = prefix_enforcer,\n",
        "        pad_token_id = tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "# Decode only the newly generated tokens\n",
        "newly_generated_tokens = outputs[0][prompt_len:]\n",
        "json_part = tokenizer.decode(newly_generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────────\n",
        "# 6. Validate and Compare the Output\n",
        "# ───────────────────────────────────────────────\n",
        "print(\"✅ Model Generated JSON:\")\n",
        "try:\n",
        "    # The output from the model should be the JSON content of the function call\n",
        "    json_part = json_part.replace(\"<|eot_id|>\", \"\").strip()\n",
        "\n",
        "    # Validate the JSON against our Pydantic schema\n",
        "    validated_data = LogSynopsis.model_validate_json(json_part)\n",
        "\n",
        "    # If successful, print the nicely formatted arguments\n",
        "    print(validated_data.model_dump_json(indent=2))\n",
        "\n",
        "except (json.JSONDecodeError, ValueError) as e:\n",
        "    print(f\"Error parsing or validating generated JSON: {e}\")\n",
        "    print(f\"Raw model output: {json_part}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"---\"*20 + \"\\n\")\n",
        "\n",
        "print(\"🎯 Ground Truth JSON:\")\n",
        "print(json.dumps(ground_truth_output, indent=2, ensure_ascii=False))\n"
      ],
      "metadata": {
        "id": "JKoNoXMrqz1T",
        "outputId": "76155596-fe87-462b-a5ee-25453b664eb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.8.1: Fast Llama patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "✅ Model Generated JSON:\n",
            "Error parsing or validating generated JSON: 1 validation error for LogSynopsis\n",
            "  Invalid JSON: EOF while parsing a value at line 90 column 49 [type=json_invalid, input_value='{\\n    \"summary\": {\\n   ...\"Web Server Responses\",', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "Raw model output: {\n",
            "    \"summary\": {\n",
            "        \"time_range\": \"2025/07/28 11:51-11:52 IST\",\n",
            "        \"status_codes\": [\n",
            "           {\n",
            "           \"description\": \"Successful Responses\",\n",
            "           \"code\": 200,\n",
            "           \"count\": 18,\n",
            "           \"bytes\": 3614991\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Client Error Responses\",\n",
            "           \"code\": 400,\n",
            "           \"count\": 1,\n",
            "           \"bytes\": 445\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Client Error Responses\",\n",
            "           \"code\": 404,\n",
            "           \"count\": 1,\n",
            "           \"bytes\": 443\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Client Error Responses\",\n",
            "           \"code\": 500,\n",
            "           \"count\": 1,\n",
            "           \"bytes\": 433\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Server Error Responses\",\n",
            "           \"code\": 502,\n",
            "           \"count\": 1,\n",
            "           \"bytes\": 445\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Redirect Responses\",\n",
            "           \"code\": 301,\n",
            "           \"count\": 6,\n",
            "           \"bytes\": 3869\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Redirect Responses\",\n",
            "           \"code\": 302,\n",
            "           \"count\": 5,\n",
            "           \"bytes\": 3875\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Web Server Responses\",\n",
            "           \"code\": 200,\n",
            "           \"count\": 1,\n",
            "           \"bytes\": 1652\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Web Server Responses\",\n",
            "           \"code\": 301,\n",
            "           \"count\": 1,\n",
            "           \"bytes\": 579\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Web Server Responses\",\n",
            "           \"code\": 302,\n",
            "           \"count\": 1,\n",
            "           \"bytes\": 778\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Web Server Responses\",\n",
            "           \"code\": 200,\n",
            "           \"count\": 1,\n",
            "           \"bytes\": 172066\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Web Server Responses\",\n",
            "           \"code\": 200,\n",
            "           \"count\": 1,\n",
            "           \"bytes\": 228845\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Web Server Responses\",\n",
            "           \"code\": 200,\n",
            "           \"count\": 1,\n",
            "           \"bytes\": 226161\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Web Server Responses\",\n",
            "           \"code\": 200,\n",
            "           \"count\": 1,\n",
            "           \"bytes\": 226161\n",
            "           },\n",
            "           {\n",
            "           \"description\": \"Web Server Responses\",\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "🎯 Ground Truth JSON:\n",
            "{\n",
            "  \"summary\": {\n",
            "    \"time_range\": \"28/Jul/2025 11:51:58 - 28/Jul/2025 11:52:13\",\n",
            "    \"status_codes\": [\n",
            "      {\n",
            "        \"code\": 200,\n",
            "        \"count\": 21,\n",
            "        \"description\": \"Successful requests\"\n",
            "      },\n",
            "      {\n",
            "        \"code\": 301,\n",
            "        \"count\": 8,\n",
            "        \"description\": \"Moved Permanently\"\n",
            "      },\n",
            "      {\n",
            "        \"code\": 302,\n",
            "        \"count\": 5,\n",
            "        \"description\": \"Found\"\n",
            "      },\n",
            "      {\n",
            "        \"code\": 400,\n",
            "        \"count\": 0,\n",
            "        \"description\": \"Bad Request\"\n",
            "      },\n",
            "      {\n",
            "        \"code\": 404,\n",
            "        \"count\": 0,\n",
            "        \"description\": \"Not Found\"\n",
            "      }\n",
            "    ],\n",
            "    \"traffic_sources\": null,\n",
            "    \"resource_types\": null,\n",
            "    \"admin_activity\": null,\n",
            "    \"observations\": null,\n",
            "    \"recommendations\": [\n",
            "      \"Monitor the 301 and 302 status codes to ensure they are intentional and necessary.\",\n",
            "      \"Check the user-agent strings for potential bot activity and adjust crawling rules if necessary.\"\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vlFqre12iwMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}